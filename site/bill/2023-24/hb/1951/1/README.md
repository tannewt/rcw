[wa-law.org](/) > [bill](/bill/) > [2023-24](/bill/2023-24/) > [HB 1951](/bill/2023-24/hb/1951/) > [Original Bill](/bill/2023-24/hb/1951/1/)

# HB 1951 - Algorithmic discrimination

[Source](http://lawfilesext.leg.wa.gov/biennium/2023-24/Pdf/Bills/House%20Bills/1951.pdf)

## Section 1
The definitions in this section apply throughout this chapter unless the context clearly requires otherwise.

1. "Algorithmic discrimination" means the condition in which an automated decision tool contributes to unjustified differential treatment or impacts disfavoring people on the basis of race, color, national origin, citizen or immigration status, families with children, creed, religious belief or affiliation, sex, marital status, the presence of any sensory, mental, or physical disability, age, honorably discharged veteran or military status, sexual orientation, gender expression or gender identity, or any other protected class under RCW 49.60.010.

2. "Artificial intelligence" means a machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations, or decisions influencing a real or virtual environment.

3. "Automated decision tool" means a system or service that uses artificial intelligence and has been specifically developed and marketed to, or specifically modified to, make, or be a controlling factor in making, consequential decisions.

4. "Consequential decision" means a decision or judgment that has a legal, material, or similarly significant effect on a natural person's life relating to the impact of, access to, or the cost, terms, or availability of, any of the following:

    a. Employment, workers management, or self-employment including, but not limited to:

        i. Pay or promotion;

        ii. Hiring or termination; and

        iii. Automated task allocation that automatically limits, segregates, or classifies employees based on individual behavior or performance for the purpose of assigning or determining material terms or conditions of employment;

    b. Education and vocational training including, but not limited to:

        i. Assessment including, but not limited to, detecting student cheating or plagiarism;

        ii. Accreditation;

        iii. Certification;

        iv. Admissions; and

    v. Financial aid or scholarships;

    c. Housing or lodging, including rental or short-term housing or lodging;

    d. Essential utilities, including electricity, heat, water, internet or telecommunications access, or transportation;

    e. Family planning, including adoption services or reproductive services, as well as assessments related to child protective services;

    f. Health care or health insurance, including mental health care, dental, or vision;

    g. Financial services, including a financial service provided by a mortgage company, mortgage broker, or creditor;

    h. The criminal justice system including, but not limited to, risk assessments for pretrial hearings, sentencing, and parole;

        i. Legal services, including private arbitration or mediation;

    j. Voting; and

    k. Access to benefits or services or assignment of penalties.

5. "Deployer" means a natural person, partnership, state or local government agency, or corporation that uses or modifies an automated decision tool to make a consequential decision.

6. "Developer" means a natural person, partnership, state or local government agency, or corporation that designs, codes, or produces an automated decision tool, or substantially modifies an artificial intelligence system or service for the known intended purpose of making, or being a controlling factor in making, consequential decisions, whether for its own use or for use by the deployer.

7. "Ethical artificial intelligence" means automated decision tools that are developed and deployed with reasonable efforts by the developer and the deployer to:

    a. Minimize unlawful discriminatory or biased outputs or applications;

    b. Ensure that automated decision tools are being operated reliably, safely, and consistently;

    c. Protect the data of natural persons by incorporating robust privacy and data security measures;

    d. Prioritize transparency so that the behavior and functional components of automated decision tools can be understood in order to enable the identification of performance issues, safety and privacy concerns, biases, exclusionary practices, and unintended outcomes; and

    e. Promote individual rights and minimize reasonably foreseeable harm to natural persons resulting from use of the automated decision tool.

8. "Impact assessment" means a documented risk-based evaluation of an automated decision tool that meets the criteria of this chapter.

9. "Sex" includes pregnancy, childbirth, and related conditions, gender identity, intersex status, and sexual orientation.

10. "Significant update" means a new version, new release, or other update to an automated decision tool that materially changes its principal use, principal intended use, or expected outcome.

## Section 2
1. By January 1, 2025, and annually thereafter, a deployer of an automated decision tool must complete and document an impact assessment for any automated decision tool the deployer uses that includes all of the following:

    a. A statement of the purpose of the automated decision tool and its intended benefits, uses, and deployment contexts;

    b. A description of the automated decision tool's outputs and how they are used to make, or be a controlling factor in making, a consequential decision;

    c. A summary of the types of data collected from natural persons and processed by the automated decision tool when it is used to make, or be a controlling factor in making, a consequential decision;

    d. A statement of the extent to which the deployer's use of the automated decision tool is consistent with or varies from the statement required of the developer by section 3 of this act;

    e. An assessment of the reasonably foreseeable risks of algorithmic discrimination arising from the use of the automated decision tool known to the deployer at the time of the impact assessment;

    f. A description of the safeguards implemented, or that will be implemented, by the deployer to align use of the automated decision tool with principles of ethical artificial intelligence and to address any reasonably foreseeable risks of algorithmic discrimination arising from the use of the automated decision tool;

    g. A description of how the automated decision tool will be used by a natural person, or monitored when it is used, to make, or be a controlling factor in making, a consequential decision; and

    h. A description of how the automated decision tool has been or will be evaluated for validity or relevance.

2. By January 1, 2025, and annually thereafter, a developer of an automated decision tool must complete and document an impact assessment of any automated decision tool that it designs, codes, or produces that includes all of the following:

    a. A statement of the purpose of the automated decision tool and its intended benefits, uses, and deployment contexts;

    b. A description of the automated decision tool's outputs and how they are used, as intended, to make, or be a controlling factor in making, a consequential decision;

    c. A summary of the types of data collected from natural persons and processed by the automated decision tool when it is used to make, or be a controlling factor in making, a consequential decision;

    d. An assessment of the reasonably foreseeable risks of algorithmic discrimination arising from the intended use or foreseeable misuse of the automated decision tool;

    e. A description of the measures taken by the developer to incorporate principles of ethical artificial intelligence and to mitigate the risk known to the developer of algorithmic discrimination arising from the use of the automated decision tool; and

    f. A description of how the automated decision tool is intended to be used by a natural person, or monitored when it is used, to make, or be a controlling factor in making, a consequential decision.

3. A deployer or developer must, in addition to the impact assessment required by subsections (1) and (2) of this section, perform, as soon as feasible, an impact assessment with respect to any significant update.

4. Upon the request of the office of the attorney general, a developer or deployer must provide any impact assessment that it performed pursuant to this section to the office of the attorney general.

5. Impact assessments provided pursuant to subsection (4) of this section are confidential and exempt from disclosure under chapter 42.56 RCW.

6. This section does not apply to a deployer with fewer than 50 employees.

## Section 3
A developer must provide a deployer with a statement regarding the intended uses of the automated decision tool and documentation regarding all of the following:

1. The known limitations of the automated decision tool, including any reasonably foreseeable risks of algorithmic discrimination arising from its intended use;

2. A description of the types of data used to program or train the automated decision tool; and

3. A description of how the automated decision tool was evaluated for validity and the ability to be explained before sale or licensing.

## Section 4
A developer must make publicly available, in a readily accessible manner, a clear policy that provides a summary of both of the following:

1. The types of automated decision tools currently made available to others by the developer; and

2. How the developer manages the reasonably foreseeable risks of algorithmic discrimination that may arise from the use of the automated decision tools it currently makes available to others.

## Section 5
1. [Empty]

    a. The attorney general may bring an action in the name of the state, or as parens patriae on behalf of persons residing in the state, to enforce this chapter. For actions brought by the attorney general to enforce this chapter, a violation of this chapter is an unfair or deceptive act in trade or commerce for the purpose of applying the consumer protection act, chapter 19.86 RCW. An action to enforce this chapter may not be brought under RCW 19.86.090.

    b. The office of the attorney general, before commencing an action under the consumer protection act, chapter 19.86 RCW, must provide 45 days' written notice to a deployer or developer of the alleged violation of this chapter and provide the deployer or developer with an opportunity to cure the alleged violation. The developer or deployer may cure the noticed violation within 45 days of receiving the written notice.

2. Nothing in this chapter shall be construed to limit or otherwise affect the obligations of developers and deployers under applicable laws, rules, or regulations relating to data privacy or security.

## Section 6
1. A deployer may not use an automated decision tool that results in algorithmic discrimination.

2. A violation of this section constitutes an unfair practice under chapter 49.60 RCW, the law against discrimination. All rights and remedies under chapter 49.60 RCW, including the right to file a complaint with the human rights commission and to bring a civil action, apply.
